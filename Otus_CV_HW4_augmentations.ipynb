{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GHEqOXDSUDi"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Аугментировать данные техниками из занятия (albumentations, torchvision, imgaug...)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-14T15:20:59.119719Z",
          "start_time": "2021-05-14T15:20:58.744916Z"
        },
        "id": "RMdUhzrNSUDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c578cf8-4489-4348-c7a8-2ba49dc2bb8f"
      },
      "source": [
        "!pip install -q -U albumentations==1.2.0\n",
        "!pip install git+https://github.com/mjkvaak/ImageDataAugmentor"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▉                             | 10 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 30 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 40 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 51 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 61 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 71 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 81 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 92 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 102 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 112 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 113 kB 28.2 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/mjkvaak/ImageDataAugmentor\n",
            "  Cloning https://github.com/mjkvaak/ImageDataAugmentor to /tmp/pip-req-build-dbabrvk2\n",
            "  Running command git clone -q https://github.com/mjkvaak/ImageDataAugmentor /tmp/pip-req-build-dbabrvk2\n",
            "Requirement already satisfied: albumentations~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: opencv-python~=4.6 in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (4.6.0.66)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ImageDataAugmentor==0.0.0) (1.7.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (4.6.0.66)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (6.0)\n",
            "Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (4.1.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (2.9.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ImageDataAugmentor==0.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations~=1.2.0->ImageDataAugmentor==0.0.0) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ImageDataAugmentor==0.0.0) (2022.6)\n",
            "Building wheels for collected packages: ImageDataAugmentor\n",
            "  Building wheel for ImageDataAugmentor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ImageDataAugmentor: filename=ImageDataAugmentor-0.0.0-py3-none-any.whl size=29643 sha256=3d66e222587aa1373a96d164b70c00d360ef96259d59a03223503898ab9adfa1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9lzv5tjt/wheels/c9/bd/73/9cfa59d2393dae55bbcc30f5aa901f55fe531c66efebbc8fc3\n",
            "Successfully built ImageDataAugmentor\n",
            "Installing collected packages: ImageDataAugmentor\n",
            "Successfully installed ImageDataAugmentor-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import History\n",
        "from ImageDataAugmentor.image_data_augmentor import *\n",
        "from albumentations import (\n",
        "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
        "    Rotate\n",
        ")"
      ],
      "metadata": {
        "id": "8lXAJlv2g8Da"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-14T15:21:08.669064Z",
          "start_time": "2021-05-14T15:20:59.840587Z"
        },
        "id": "-8VFF2kySUDr"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import mobilenet\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"imagenette2\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
        "    !tar -xzf imagenette2.tgz"
      ],
      "metadata": {
        "id": "mVUu0Mm3J567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b8a6ed-c500-4b9b-e713-dd9e6a8da8aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-26 08:32:27--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.134.184, 54.231.171.48, 54.231.192.72, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.134.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1557161267 (1.5G) [application/x-tar]\n",
            "Saving to: ‘imagenette2.tgz’\n",
            "\n",
            "imagenette2.tgz     100%[===================>]   1.45G  12.5MB/s    in 1m 58s  \n",
            "\n",
            "2022-11-26 08:34:27 (12.6 MB/s) - ‘imagenette2.tgz’ saved [1557161267/1557161267]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RPnQA9WSUDt"
      },
      "source": [
        "Подгружаем датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-14T15:21:28.065365Z",
          "start_time": "2021-05-14T15:21:28.061750Z"
        },
        "id": "nRjxTJFKSUDu"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_path = Path('/content/imagenette2/train/')\n",
        "val_path = Path('/content/imagenette2/val/')\n",
        "accuracy = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ф-ция top 5 accuracy:"
      ],
      "metadata": {
        "id": "iFWcGLxgFw8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(y_pred, y_true, k):\n",
        "  true = 0\n",
        "  for idx in range(len(y_true)):    \n",
        "    if y_true[idx] in y_pred[idx][:k]:\n",
        "      true += 1  \n",
        "  return (true / len(y_true))"
      ],
      "metadata": {
        "id": "qEkFb7bv3lvx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = mobilenet.MobileNet(weights='imagenet')\n",
        "helper_module = mobilenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH3EZ5F2d5wr",
        "outputId": "d5e71f43-73c1-45c5-f722-6984ef182f5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17225924/17225924 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_datagen = ImageDataGenerator(preprocessing_function=helper_module.preprocess_input)\n",
        "valid_gen = valid_datagen.flow_from_directory(\n",
        "  val_path,\n",
        "  shuffle=False, seed=42, \n",
        "  target_size=model.input_shape[1:-1],\n",
        "  batch_size=64\n",
        ")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngiQmEENeAbv",
        "outputId": "8f5a38c2-1e8b-4e1c-ffb7-bca695f55772"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3925 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисляем top 5 accuracy"
      ],
      "metadata": {
        "id": "SnN3LzE6F5um"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-14T15:21:34.392943Z",
          "start_time": "2021-05-14T15:21:34.390828Z"
        },
        "id": "hzi5HVxNSUDv"
      },
      "source": [
        "def model_top_5_accuracy(model, valid_gen):\n",
        "\n",
        "  top_k_acc = [] \n",
        "  count = 1\n",
        "  for batch_x, batch_y in valid_gen:\n",
        "    preds = model.predict(batch_x, steps=1, verbose=0)\n",
        "    preds_decoded = helper_module.decode_predictions(preds, top=5)\n",
        "    preds_decoded = np.array(preds_decoded)\n",
        "    preds_decoded.shape\n",
        "\n",
        "    y_true_indexs = np.where(batch_y == 1)[1]\n",
        "    y_true_labels = []\n",
        "    for idx in y_true_indexs:\n",
        "       y_true_labels.append(list(valid_gen.class_indices.items())[idx][0])\n",
        "\n",
        "    top_k_acc.append(top_k_accuracy(preds_decoded, y_true_labels, 5))\n",
        "\n",
        "    count += 1\n",
        "    if count > valid_gen.__len__():\n",
        "      break\n",
        "    \n",
        "  return mean(top_k_acc)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy.append(model_top_5_accuracy(model, valid_gen))"
      ],
      "metadata": {
        "id": "KyDBmbLOezFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bb9841-ca68-4538-dd02-e7973ca0da75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUGMENTATIONS1 = albumentations.Compose([\n",
        "    albumentations.Flip(p=0.5),\n",
        "    albumentations.GaussianBlur(p=0.05),\n",
        "])\n",
        "\n",
        "AUGMENTATIONS2 = albumentations.Compose([\n",
        "    albumentations.Flip(p=0.5),\n",
        "    albumentations.GaussianBlur(p=0.05),\n",
        "    albumentations.RandomCrop(height=192, width=192),\n",
        "    albumentations.HueSaturationValue(p=0.5),\n",
        "])\n",
        "\n",
        "AUGMENTATIONS3 = albumentations.Compose([\n",
        "    albumentations.RandomCrop(height=192, width=192),\n",
        "    albumentations.OneOf([\n",
        "        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
        "        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n",
        "    ],p=1),\n",
        "    albumentations.HueSaturationValue(p=0.5),\n",
        "])"
      ],
      "metadata": {
        "id": "lIiUQNBxIbNX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model_with_fc_layer(base_model):\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(1000, activation='relu')(x)\n",
        "  predictions = Dense(10, activation='softmax')(x)\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=5)])\n",
        "  return model"
      ],
      "metadata": {
        "id": "bTviyHx7-c8F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "model = update_model_with_fc_layer(base_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUbh6UoA5ZPs",
        "outputId": "53181373-c33e-416e-de4c-5dd789b132d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment=AUGMENTATIONS1,\n",
        "        preprocess_input=None)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        shuffle=True, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)\n",
        "val_datagen = ImageDataAugmentor(rescale=1./255)\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        shuffle=False, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAwZDHPaDqVc",
        "outputId": "5008f8d8-ca13-4eae-9772-3212d842a788"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ImageDataAugmentor/image_data_augmentor.py:333: UserWarning: Passing `seed` in `.flow_from_directory` has been been removed: pass  `seed` as parameter in `ImageDataAugmentor(..., seed=...)` instead\n",
            "  warnings.warn('Passing `seed` in `.flow_from_directory` has been been removed: pass  `seed` '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9469 images belonging to 10 classes.\n",
            "Found 3925 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch=len(train_generator),\n",
        "  epochs=10,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=len(validation_generator))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxEak7y3z-QM",
        "outputId": "37e1b1ce-72ee-47d8-d478-adc9b5dd9d95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "148/148 [==============================] - 102s 676ms/step - loss: 0.2253 - top_k_categorical_accuracy: 0.9879 - val_loss: 0.0526 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "148/148 [==============================] - 99s 675ms/step - loss: 0.0798 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0374 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "148/148 [==============================] - 100s 683ms/step - loss: 0.0721 - top_k_categorical_accuracy: 0.9995 - val_loss: 0.0484 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "148/148 [==============================] - 100s 679ms/step - loss: 0.0570 - top_k_categorical_accuracy: 0.9995 - val_loss: 0.0638 - val_top_k_categorical_accuracy: 0.9995\n",
            "Epoch 5/10\n",
            "148/148 [==============================] - 100s 682ms/step - loss: 0.0619 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0502 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "148/148 [==============================] - 99s 676ms/step - loss: 0.0500 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0562 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "148/148 [==============================] - 98s 669ms/step - loss: 0.0405 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0455 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "148/148 [==============================] - 99s 675ms/step - loss: 0.0387 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0809 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "148/148 [==============================] - 99s 676ms/step - loss: 0.0319 - top_k_categorical_accuracy: 0.9999 - val_loss: 0.0513 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "148/148 [==============================] - 98s 668ms/step - loss: 0.0348 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0513 - val_top_k_categorical_accuracy: 0.9997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy.append(mean(history.history['val_top_k_categorical_accuracy']))"
      ],
      "metadata": {
        "id": "UH4GRiiPDwuC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment=AUGMENTATIONS2,\n",
        "        preprocess_input=None)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        shuffle=True, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)\n",
        "val_datagen = ImageDataAugmentor(rescale=1./255)\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        shuffle=False, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)"
      ],
      "metadata": {
        "id": "hoZxVD5TDxJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebaf876-d5f9-4fc8-80ae-81123625540a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9469 images belonging to 10 classes.\n",
            "Found 3925 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "update_model_with_fc_layer(base_model)"
      ],
      "metadata": {
        "id": "Iow-hiSQEBmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f19947e-a109-4ec0-c4c6-b05a61de3345"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f46ac131a90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch=len(train_generator),\n",
        "  epochs=10,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=len(validation_generator))"
      ],
      "metadata": {
        "id": "r9o7OTeVEQGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9c57ac-02d9-489a-8419-df569fe04ae9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "148/148 [==============================] - 95s 642ms/step - loss: 0.1208 - top_k_categorical_accuracy: 0.9993 - val_loss: 0.0551 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "148/148 [==============================] - 94s 636ms/step - loss: 0.0923 - top_k_categorical_accuracy: 0.9995 - val_loss: 0.0533 - val_top_k_categorical_accuracy: 0.9995\n",
            "Epoch 3/10\n",
            "148/148 [==============================] - 95s 644ms/step - loss: 0.0814 - top_k_categorical_accuracy: 0.9994 - val_loss: 0.0595 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "148/148 [==============================] - 93s 634ms/step - loss: 0.0715 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0449 - val_top_k_categorical_accuracy: 0.9995\n",
            "Epoch 5/10\n",
            "148/148 [==============================] - 93s 636ms/step - loss: 0.0615 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0339 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "148/148 [==============================] - 94s 636ms/step - loss: 0.0626 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0437 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "148/148 [==============================] - 92s 628ms/step - loss: 0.0533 - top_k_categorical_accuracy: 0.9997 - val_loss: 0.0421 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 8/10\n",
            "148/148 [==============================] - 93s 634ms/step - loss: 0.0463 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0474 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 9/10\n",
            "148/148 [==============================] - 92s 627ms/step - loss: 0.0524 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0482 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 10/10\n",
            "148/148 [==============================] - 93s 634ms/step - loss: 0.0481 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.0395 - val_top_k_categorical_accuracy: 0.9997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy.append(mean(history.history['val_top_k_categorical_accuracy']))"
      ],
      "metadata": {
        "id": "p9XuTpQfEUYc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment=AUGMENTATIONS3,\n",
        "        preprocess_input=None)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        shuffle=True, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)\n",
        "val_datagen = ImageDataAugmentor(rescale=1./255)\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        shuffle=False, seed=42, \n",
        "        #target_size=model.input_shape[1:-1],\n",
        "        batch_size=64)"
      ],
      "metadata": {
        "id": "J7MLN_8KEHrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae66542-4ba2-417d-fc65-23e9cf7cfe96"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9469 images belonging to 10 classes.\n",
            "Found 3925 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "update_model_with_fc_layer(base_model)"
      ],
      "metadata": {
        "id": "d5f986mbEaAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1ab9d8-f6f7-4e67-b9d6-3eb20c8148aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f46962626d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch=len(train_generator),\n",
        "  epochs=10,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=len(validation_generator))"
      ],
      "metadata": {
        "id": "ucmj76ZHEgAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0a81fb-5dc1-42fe-bb59-faed32b104a1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "148/148 [==============================] - 98s 664ms/step - loss: 0.0247 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0459 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 2/10\n",
            "148/148 [==============================] - 110s 746ms/step - loss: 0.0220 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0440 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 3/10\n",
            "148/148 [==============================] - 97s 658ms/step - loss: 0.0194 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0540 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 4/10\n",
            "148/148 [==============================] - 98s 665ms/step - loss: 0.0185 - top_k_categorical_accuracy: 0.9999 - val_loss: 0.0460 - val_top_k_categorical_accuracy: 0.9995\n",
            "Epoch 5/10\n",
            "148/148 [==============================] - 98s 664ms/step - loss: 0.0161 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0484 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 6/10\n",
            "148/148 [==============================] - 109s 742ms/step - loss: 0.0154 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0504 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 7/10\n",
            "148/148 [==============================] - 96s 655ms/step - loss: 0.0138 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.0662 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "148/148 [==============================] - 108s 734ms/step - loss: 0.0234 - top_k_categorical_accuracy: 0.9999 - val_loss: 0.0361 - val_top_k_categorical_accuracy: 0.9997\n",
            "Epoch 9/10\n",
            "148/148 [==============================] - 95s 649ms/step - loss: 0.0171 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0505 - val_top_k_categorical_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "148/148 [==============================] - 109s 739ms/step - loss: 0.0117 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0687 - val_top_k_categorical_accuracy: 0.9997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy.append(mean(history.history['val_top_k_categorical_accuracy']))"
      ],
      "metadata": {
        "id": "oahZBynTEg9z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = [\"Without augmentations\", \"Flip, GaussianBlur\", \"Flip, GaussianBlur, RandomCrop, HueSaturation\", \"RandomCrop, BrightnessContrast, HueSaturation\"]"
      ],
      "metadata": {
        "id": "Rs2_GBOvEj_m"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'Augmentations': augmentations, 'Top 5 accuracy': accuracy}\n",
        "df = pd.DataFrame(data=d)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "B9pdS1EiO56V",
        "outputId": "e48ef66a-abe1-4df9-b072-7cab8e508d06"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   Augmentations  Top 5 accuracy\n",
              "0                          Without augmentations        0.980583\n",
              "1                             Flip, GaussianBlur        0.999924\n",
              "2  Flip, GaussianBlur, RandomCrop, HueSaturation        0.999796\n",
              "3  RandomCrop, BrightnessContrast, HueSaturation        0.999771"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1c452fc-d1bf-45be-922c-18f102320900\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Augmentations</th>\n",
              "      <th>Top 5 accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Without augmentations</td>\n",
              "      <td>0.980583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flip, GaussianBlur</td>\n",
              "      <td>0.999924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Flip, GaussianBlur, RandomCrop, HueSaturation</td>\n",
              "      <td>0.999796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomCrop, BrightnessContrast, HueSaturation</td>\n",
              "      <td>0.999771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1c452fc-d1bf-45be-922c-18f102320900')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1c452fc-d1bf-45be-922c-18f102320900 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1c452fc-d1bf-45be-922c-18f102320900');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}